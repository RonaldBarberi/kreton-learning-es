# Statistics - Data Science

## Tabla General de modelos de Machine Learning

| **Tipo de problema**                            | **Modelo**                                            | **Descripción / Cómo funciona**                                       | **Cuándo usarlo (escenario ideal)**                                                                                                    | **Ventajas principales**                                       | **Desventajas / Limitaciones**                      |
| ----------------------------------------------- | ----------------------------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | --------------------------------------------------- |
| **Clasificación (Binaria o Multiclase)**        | **Regresión Logística**                               | Modelo lineal que estima probabilidades mediante la función sigmoide. | Cuando las relaciones entre variables son simples y necesitas interpretar los coeficientes (por ejemplo, propensión de compra, churn). | Interpretación clara, rápido de entrenar.                      | No capta relaciones no lineales.                    |
|                                                 | **K-Nearest Neighbors (KNN)**                         | Clasifica según la mayoría de los vecinos más cercanos.               | Cuando las clases están bien separadas y el dataset es pequeño.                                                                        | Simple y sin entrenamiento complejo.                           | Escala mal con grandes volúmenes; sensible a ruido. |
|                                                 | **Árbol de decisión**                                 | Divide los datos en reglas jerárquicas.                               | Cuando quieres reglas interpretables (por ejemplo: “si edad > 40 y región = norte…”).                                                  | Interpretación visual, maneja variables mixtas.                | Tiende al overfitting si no se regula.              |
|                                                 | **Random Forest**                                     | Conjunto de árboles de decisión combinados por voto.                  | Cuando buscas precisión y estabilidad con datos mixtos.                                                                                | Robusto, maneja no linealidad y ruido.                         | Menos interpretable.                                |
|                                                 | **Gradient Boosting / XGBoost / LightGBM / CatBoost** | Ensambles secuenciales de árboles que corrigen errores previos.       | Cuando necesitas máximo rendimiento (predicción de compra, crédito, riesgo, fraude).                                                   | Precisión muy alta, manejo eficiente de categorías (CatBoost). | Más complejo de ajustar.                            |
|                                                 | **SVM (Support Vector Machine)**                      | Busca un hiperplano que separe las clases con máximo margen.          | Cuando las clases no son linealmente separables y el dataset no es masivo.                                                             | Preciso en datasets medianos.                                  | Lento con muchos datos y difícil de interpretar.    |
|                                                 | **Red neuronal (MLP)**                                | Capas de nodos que aprenden patrones complejos.                       | Cuando tienes gran cantidad de datos estructurados o no lineales.                                                                      | Captura patrones complejos.                                    | Requiere más datos y tuning.                        |
| **Regresión (Predicción de valores continuos)** | **Regresión Lineal**                                  | Modelo simple que ajusta una recta o hiperplano.                      | Cuando la relación entre variables es lineal (por ejemplo, predicción de ventas o precios).                                            | Fácil de interpretar y rápido.                                 | No maneja bien relaciones no lineales.              |
|                                                 | **Regresión Ridge / Lasso / ElasticNet**              | Variantes de regresión lineal con regularización.                     | Cuando hay muchas variables correlacionadas.                                                                                           | Evitan overfitting.                                            | Siguen siendo lineales.                             |
|                                                 | **Árbol de regresión / Random Forest Regressor**      | Árboles adaptados para valores continuos.                             | Cuando hay relaciones no lineales entre variables.                                                                                     | Robustos y automáticos.                                        | Menos interpretables.                               |
|                                                 | **Gradient Boosting Regressor / XGBoost Regressor**   | Versión de boosting para regresión.                                   | Cuando necesitas alta precisión (por ejemplo, estimar ingresos).                                                                       | Precisión alta y flexible.                                     | Requiere ajuste de hiperparámetros.                 |
|                                                 | **SVR (Support Vector Regression)**                   | Versión de SVM para regresión.                                        | Cuando tienes pocos datos y relaciones complejas.                                                                                      | Preciso en datasets pequeños.                                  | Lento en datasets grandes.                          |
|                                                 | **Red neuronal (Regresión)**                          | Aprende funciones no lineales.                                        | Cuando hay patrones complejos y muchos datos.                                                                                          | Flexible.                                                      | Costosa computacionalmente.                         |
| **Agrupamiento (Clustering / No supervisado)**  | **K-Means**                                           | Agrupa puntos según cercanía (centroides).                            | Segmentación de clientes, comportamiento de compra.                                                                                    | Simple y rápido.                                               | Requiere definir el número de grupos.               |
|                                                 | **Hierarchical Clustering**                           | Agrupa de forma jerárquica sin definir K.                             | Cuando se necesita dendrograma o estructura jerárquica.                                                                                | No necesita K inicial.                                         | Costoso con grandes datasets.                       |
|                                                 | **DBSCAN**                                            | Agrupa por densidad.                                                  | Cuando hay clusters irregulares y ruido.                                                                                               | Detecta outliers.                                              | Sensible a parámetros.                              |
| **Reducción de dimensionalidad**                | **PCA (Análisis de Componentes Principales)**         | Reduce variables manteniendo máxima varianza.                         | Visualización o compresión de datos con muchas variables.                                                                              | Mejora rendimiento y reduce ruido.                             | Difícil de interpretar.                             |
|                                                 | **t-SNE / UMAP**                                      | Métodos no lineales para visualizar datos.                            | Exploración y visualización 2D/3D.                                                                                                     | Captan estructura compleja.                                    | No sirven para predicción.                          |
| **Series de tiempo**                            | **ARIMA / SARIMA**                                    | Modelos estadísticos basados en autocorrelación.                      | Predicciones de demanda, ventas, tráfico.                                                                                              | Interpretables.                                                | Requieren estacionariedad.                          |
|                                                 | **Prophet (Facebook)**                                | Modelo aditivo flexible con estacionalidad.                           | Forecasts de negocio con tendencias y estacionalidad.                                                                                  | Fácil de usar.                                                 | Menos preciso con ruido alto.                       |
|                                                 | **LSTM / GRU (Deep Learning)**                        | Redes neuronales recurrentes para secuencias.                         | Series largas y no lineales (por ejemplo, predicción de consumo).                                                                      | Capta dependencias temporales.                                 | Requiere muchos datos.                              |
| **Detección de anomalías**                      | **Isolation Forest**                                  | Aísla valores atípicos mediante árboles.                              | Detección de fraude o errores en sensores.                                                                                             | Escalable, preciso.                                            | Difícil interpretar anomalías.                      |
|                                                 | **One-Class SVM**                                     | Separa puntos normales de anomalías.                                  | Detección de valores raros.                                                                                                            | Eficiente en alta dimensionalidad.                             | Sensible a parámetros.                              |
| **Recomendación**                               | **Collaborative Filtering / Matrix Factorization**    | Predice gustos basándose en usuarios similares.                       | Recomendadores tipo Netflix o Spotify.                                                                                                 | Personaliza sugerencias.                                       | Necesita muchos datos históricos.                   |
| **Procesamiento de texto (NLP)**                | **Naive Bayes**                                       | Basado en probabilidad condicional.                                   | Clasificación de textos (spam/no spam).                                                                                                | Rápido, simple.                                                | Supone independencia de palabras.                   |
|                                                 | **TF-IDF + SVM / Logistic**                           | Usa pesos de palabras y modelos lineales.                             | Análisis de sentimientos, clasificaciones simples.                                                                                     | Buen rendimiento general.                                      | Limitado en contexto.                               |
|                                                 | **Transformers (BERT, GPT, etc.)**                    | Modelos preentrenados de lenguaje.                                    | Tareas avanzadas de texto (chatbots, QA, resumen).                                                                                     | Potentes y precisos.                                           | Costosos de entrenar.                               |
| **Visión por computadora**                      | **CNN (Convolutional Neural Network)**                | Aprende características visuales.                                     | Clasificación de imágenes, reconocimiento facial.                                                                                      | Muy precisas.                                                  | Requieren GPU y grandes datasets.                   |

---
## Guía rápida de elección
| **Tipo de dato / tarea**               | **Modelo recomendado**             |
| -------------------------------------- | ---------------------------------- |
| Predicción de compra, abandono, fraude | Random Forest / XGBoost / CatBoost |
| Predicción de precios o montos         | Gradient Boosting Regressor        |
| Segmentación de clientes               | K-Means                            |
| Análisis de texto                      | Naive Bayes / Transformers         |
| Predicción de demanda / ventas         | Prophet / LSTM                     |
| Imágenes o video                       | CNN                                |
| Datos mixtos con muchas categorías     | CatBoost                           |
| Datos lineales y pocos                 | Regresión logística o lineal       |

---
## Matriz de confusión
|               | Predicho Positivo   | Predicho Negativo   |
| ------------- | ------------------- | ------------------- |
| Real Positivo | TP (True Positive)  | FN (False Negative) |
| Real Negativo | FP (False Positive) | TN (True Negative)  |

---
Metricas de medición para modelos ML
| Métrica                      | Fórmula                                                             | Interpretación                                 | Rango  | Comentario de eficiencia                |
| ---------------------------- | ------------------------------------------------------------------- | ---------------------------------------------- | ------ | --------------------------------------- |
| **Accuracy**                 | $( \frac{TP + TN}{TP + TN + FP + FN} )$                             | Proporción de aciertos globales                | [0, 1] | Sensible al desbalance de clases        |
| **Precision**                | $( \frac{TP}{TP + FP} )$                                            | Fiabilidad de las predicciones positivas       | [0, 1] | Útil en contextos con alto costo de FP  |
| **Recall (Sensibilidad)**    | $( \frac{TP}{TP + FN} )$                                            | Capacidad de detectar positivos reales         | [0, 1] | Clave en contextos con alto costo de FN |
| **F1-score**                 | $( 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall} )$     | Media armónica de precisión y recall           | [0, 1] | Balancea falsos positivos y negativos   |
| **AUC-ROC**                  | Área bajo la curva ROC                                              | Evalúa capacidad de discriminación             | [0, 1] | Costoso: O(n log n) para ordenamiento   |
| **Log-loss / Cross-entropy** | $( -\frac{1}{n}\sum [y_i \log(\hat{p}_i) + (1-y_i)\log(1-\hat{p}_i)] )$ | Penaliza predicciones probabilísticas erróneas | [0, ∞) | Estable en modelos probabilísticos  |
| **Balanced Accuracy**        | Promedio de recall por clase                                        | Corrige desbalance de clases                   | [0, 1] | Adecuado para datasets desbalanceados   |

---
## Metricas de Regresión
| Métrica                               | Fórmula                                                           | Interpretación                         | Rango   |                           |        |
| ------------------------------------- | ----------------------------------------------------------------- | -------------------------------------- | ------- | ------------------------- | ------ |
| **MAE (Mean Absolute Error)**         | $( \frac{1}{n}\sum$                                               | y_i - \hat{y}_i                        | )       | Error promedio absoluto   | [0, ∞) |
| **MSE (Mean Squared Error)**          | $( \frac{1}{n}\sum (y_i - \hat{y}_i)^2 )$                         | Penaliza grandes errores               | [0, ∞)  |                           |        |
| **RMSE**                              | $( \sqrt{MSE} )$                                                  | Error cuadrático en misma escala que y | [0, ∞)  |                           |        |
| **R² (Coeficiente de determinación)** | $( 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} )$ | Proporción de varianza explicada       | (-∞, 1] |                           |        |
| **MAPE**                              | $( \frac{100}{n}\sum$                                      | \frac{y_i - \hat{y}_i}{y_i}\right      | )       | Error relativo porcentual | [0, ∞) |

---
## Metricas de Clustering
| Tipo    | Métrica                                                                               | Descripción                            |        |
| ------- | ------------------------------------------------------------------------------------- | -------------------------------------- | ------ |
| Interna | **Silhouette Score**: mide cohesión y separación de clusters.                         | Alto = mejor                           |        |
| Interna | **Calinski–Harabasz**: ratio entre dispersión inter e intra-cluster.                  | Alto = mejor                           |        |
| Interna | **Davies–Bouldin Index**: media de similitud entre clusters.                          | Bajo = mejor                           |        |
| Externa | **Adjusted Rand Index (ARI)**: compara coincidencia de etiquetas reales vs predichas. | [-1, 1]                                |        |
| Externa | **Normalized Mutual Information (NMI)**                                               | Entropía compartida entre particiones. | [0, 1] |

---
## Metricas de Ranking
| Métrica                                          | Descripción                                               | Uso típico                |
| ------------------------------------------------ | --------------------------------------------------------- | ------------------------- |
| **Precision@k / Recall@k**                       | Evalúa relevancia de los primeros *k* ítems recomendados. | Sistemas de recomendación |
| **MAP (Mean Average Precision)**                 | Promedio de la precisión en cada posición relevante.      | Ranking ordenado          |
| **NDCG (Normalized Discounted Cumulative Gain)** | Penaliza errores en posiciones altas.                     | Motores de búsqueda       |

