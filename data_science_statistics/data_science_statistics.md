# Statistics - Data Science

## Tabla General de modelos de Machine Learning

| **Tipo de problema**                            | **Modelo**                                            | **Descripción / Cómo funciona**                                       | **Cuándo usarlo (escenario ideal)**                                                                                                    | **Ventajas principales**                                       | **Desventajas / Limitaciones**                      |
| ----------------------------------------------- | ----------------------------------------------------- | --------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------- | --------------------------------------------------- |
| **Clasificación (Binaria o Multiclase)**        | **Regresión Logística**                               | Modelo lineal que estima probabilidades mediante la función sigmoide. | Cuando las relaciones entre variables son simples y necesitas interpretar los coeficientes (por ejemplo, propensión de compra, churn). | Interpretación clara, rápido de entrenar.                      | No capta relaciones no lineales.                    |
|                                                 | **K-Nearest Neighbors (KNN)**                         | Clasifica según la mayoría de los vecinos más cercanos.               | Cuando las clases están bien separadas y el dataset es pequeño.                                                                        | Simple y sin entrenamiento complejo.                           | Escala mal con grandes volúmenes; sensible a ruido. |
|                                                 | **Árbol de decisión**                                 | Divide los datos en reglas jerárquicas.                               | Cuando quieres reglas interpretables (por ejemplo: “si edad > 40 y región = norte…”).                                                  | Interpretación visual, maneja variables mixtas.                | Tiende al overfitting si no se regula.              |
|                                                 | **Random Forest**                                     | Conjunto de árboles de decisión combinados por voto.                  | Cuando buscas precisión y estabilidad con datos mixtos.                                                                                | Robusto, maneja no linealidad y ruido.                         | Menos interpretable.                                |
|                                                 | **Gradient Boosting / XGBoost / LightGBM / CatBoost** | Ensambles secuenciales de árboles que corrigen errores previos.       | Cuando necesitas máximo rendimiento (predicción de compra, crédito, riesgo, fraude).                                                   | Precisión muy alta, manejo eficiente de categorías (CatBoost). | Más complejo de ajustar.                            |
|                                                 | **SVM (Support Vector Machine)**                      | Busca un hiperplano que separe las clases con máximo margen.          | Cuando las clases no son linealmente separables y el dataset no es masivo.                                                             | Preciso en datasets medianos.                                  | Lento con muchos datos y difícil de interpretar.    |
|                                                 | **Red neuronal (MLP)**                                | Capas de nodos que aprenden patrones complejos.                       | Cuando tienes gran cantidad de datos estructurados o no lineales.                                                                      | Captura patrones complejos.                                    | Requiere más datos y tuning.                        |
| **Regresión (Predicción de valores continuos)** | **Regresión Lineal**                                  | Modelo simple que ajusta una recta o hiperplano.                      | Cuando la relación entre variables es lineal (por ejemplo, predicción de ventas o precios).                                            | Fácil de interpretar y rápido.                                 | No maneja bien relaciones no lineales.              |
|                                                 | **Regresión Ridge / Lasso / ElasticNet**              | Variantes de regresión lineal con regularización.                     | Cuando hay muchas variables correlacionadas.                                                                                           | Evitan overfitting.                                            | Siguen siendo lineales.                             |
|                                                 | **Árbol de regresión / Random Forest Regressor**      | Árboles adaptados para valores continuos.                             | Cuando hay relaciones no lineales entre variables.                                                                                     | Robustos y automáticos.                                        | Menos interpretables.                               |
|                                                 | **Gradient Boosting Regressor / XGBoost Regressor**   | Versión de boosting para regresión.                                   | Cuando necesitas alta precisión (por ejemplo, estimar ingresos).                                                                       | Precisión alta y flexible.                                     | Requiere ajuste de hiperparámetros.                 |
|                                                 | **SVR (Support Vector Regression)**                   | Versión de SVM para regresión.                                        | Cuando tienes pocos datos y relaciones complejas.                                                                                      | Preciso en datasets pequeños.                                  | Lento en datasets grandes.                          |
|                                                 | **Red neuronal (Regresión)**                          | Aprende funciones no lineales.                                        | Cuando hay patrones complejos y muchos datos.                                                                                          | Flexible.                                                      | Costosa computacionalmente.                         |
| **Agrupamiento (Clustering / No supervisado)**  | **K-Means**                                           | Agrupa puntos según cercanía (centroides).                            | Segmentación de clientes, comportamiento de compra.                                                                                    | Simple y rápido.                                               | Requiere definir el número de grupos.               |
|                                                 | **Hierarchical Clustering**                           | Agrupa de forma jerárquica sin definir K.                             | Cuando se necesita dendrograma o estructura jerárquica.                                                                                | No necesita K inicial.                                         | Costoso con grandes datasets.                       |
|                                                 | **DBSCAN**                                            | Agrupa por densidad.                                                  | Cuando hay clusters irregulares y ruido.                                                                                               | Detecta outliers.                                              | Sensible a parámetros.                              |
| **Reducción de dimensionalidad**                | **PCA (Análisis de Componentes Principales)**         | Reduce variables manteniendo máxima varianza.                         | Visualización o compresión de datos con muchas variables.                                                                              | Mejora rendimiento y reduce ruido.                             | Difícil de interpretar.                             |
|                                                 | **t-SNE / UMAP**                                      | Métodos no lineales para visualizar datos.                            | Exploración y visualización 2D/3D.                                                                                                     | Captan estructura compleja.                                    | No sirven para predicción.                          |
| **Series de tiempo**                            | **ARIMA / SARIMA**                                    | Modelos estadísticos basados en autocorrelación.                      | Predicciones de demanda, ventas, tráfico.                                                                                              | Interpretables.                                                | Requieren estacionariedad.                          |
|                                                 | **Prophet (Facebook)**                                | Modelo aditivo flexible con estacionalidad.                           | Forecasts de negocio con tendencias y estacionalidad.                                                                                  | Fácil de usar.                                                 | Menos preciso con ruido alto.                       |
|                                                 | **LSTM / GRU (Deep Learning)**                        | Redes neuronales recurrentes para secuencias.                         | Series largas y no lineales (por ejemplo, predicción de consumo).                                                                      | Capta dependencias temporales.                                 | Requiere muchos datos.                              |
| **Detección de anomalías**                      | **Isolation Forest**                                  | Aísla valores atípicos mediante árboles.                              | Detección de fraude o errores en sensores.                                                                                             | Escalable, preciso.                                            | Difícil interpretar anomalías.                      |
|                                                 | **One-Class SVM**                                     | Separa puntos normales de anomalías.                                  | Detección de valores raros.                                                                                                            | Eficiente en alta dimensionalidad.                             | Sensible a parámetros.                              |
| **Recomendación**                               | **Collaborative Filtering / Matrix Factorization**    | Predice gustos basándose en usuarios similares.                       | Recomendadores tipo Netflix o Spotify.                                                                                                 | Personaliza sugerencias.                                       | Necesita muchos datos históricos.                   |
| **Procesamiento de texto (NLP)**                | **Naive Bayes**                                       | Basado en probabilidad condicional.                                   | Clasificación de textos (spam/no spam).                                                                                                | Rápido, simple.                                                | Supone independencia de palabras.                   |
|                                                 | **TF-IDF + SVM / Logistic**                           | Usa pesos de palabras y modelos lineales.                             | Análisis de sentimientos, clasificaciones simples.                                                                                     | Buen rendimiento general.                                      | Limitado en contexto.                               |
|                                                 | **Transformers (BERT, GPT, etc.)**                    | Modelos preentrenados de lenguaje.                                    | Tareas avanzadas de texto (chatbots, QA, resumen).                                                                                     | Potentes y precisos.                                           | Costosos de entrenar.                               |
| **Visión por computadora**                      | **CNN (Convolutional Neural Network)**                | Aprende características visuales.                                     | Clasificación de imágenes, reconocimiento facial.                                                                                      | Muy precisas.                                                  | Requieren GPU y grandes datasets.                   |
